{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load in libraries\n",
    "import cPickle as pickle\n",
    "import urllib2\n",
    "import shutil\n",
    "from time import time\n",
    "import os\n",
    "import random\n",
    "import io\n",
    "from __future__ import print_function\n",
    "\n",
    "# ML libraries\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.decomposition import LatentDirichletAllocation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# http://stackoverflow.com/questions/10058591/how-can-i-open-utf-16-files-on-python-2-x\n",
    "\n",
    "db = pickle.load(open('db.p', 'rb'))\n",
    "txts = []\n",
    "pids = []\n",
    "n=0\n",
    "for pid,j in db.iteritems():\n",
    "  n+=1\n",
    "  fname = os.path.join('txt', pid) + '.txt'\n",
    "  if os.path.isfile(fname):\n",
    "    try:\n",
    "        txt = io.open(fname, 'r', encoding = 'utf-16').read()\n",
    "        txts.append(txt)\n",
    "    except UnicodeError:\n",
    "        txt = open(fname, 'r').read()\n",
    "        txts.append(txt)\n",
    "    pids.append(pid)\n",
    "  #print 'reading %d/%d' % (n, len(db))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "txts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing txt.p\n"
     ]
    }
   ],
   "source": [
    "out = {}\n",
    "out['text'] = txts\n",
    "out['pids'] = pids\n",
    "\n",
    "print('writing txt.p')\n",
    "pickle.dump(out, open(\"txt.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_samples = 2000\n",
    "n_features = 5000\n",
    "n_topics = 30\n",
    "n_top_words = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First, Compute TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting tf-idf features for NMF...\n",
      "done in 374.267s.\n"
     ]
    }
   ],
   "source": [
    "print(\"Extracting tf-idf features for NMF...\")\n",
    "\n",
    "tf_izer = TfidfVectorizer(input='content', \n",
    "        encoding='utf-8', decode_error='replace', strip_accents='unicode', lowercase=True, \n",
    "        analyzer='word', stop_words='english', \n",
    "        token_pattern=r'(?u)\\b[a-zA-Z_][a-zA-Z0-9_]+\\b',\n",
    "        ngram_range=(1, 2), max_features = 5000, \n",
    "        norm='l2', use_idf=True, smooth_idf=True, sublinear_tf=False)\n",
    "\n",
    "t0 = time()\n",
    "tfidf = tf_izer.fit_transform(txts)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"Topic #%d:\" % topic_idx)\n",
    "        print(\" \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-n_top_words - 1:-1]]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Fit the NMF model\n",
    "print(\"Fitting the NMF model with tf-idf features,\"\n",
    "      \"n_samples=%d and n_features=%d...\"\n",
    "      % (n_samples, n_features))\n",
    "t0 = time()\n",
    "nmf = NMF(n_components=n_topics, random_state=1, alpha=.1, l1_ratio=.5).fit(tfidf)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "\n",
    "print(\"\\nTopics in NMF model:\")\n",
    "tfidf_feature_names = tf_izer.get_feature_names()\n",
    "print_top_words(nmf, tfidf_feature_names, n_top_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 398.395s.\n"
     ]
    }
   ],
   "source": [
    "# Use tf (raw term count) features for LDA.\n",
    "t0 = time()\n",
    "count_izer = CountVectorizer(input='content',\n",
    "                             encoding='utf-8', decode_error='replace', strip_accents='unicode', lowercase=True, \n",
    "                             analyzer='word', stop_words='english',\n",
    "                             token_pattern=r'(?u)\\b[a-zA-Z_][a-zA-Z0-9_]+\\b',\n",
    "                             ngram_range=(1, 2), max_features = 5000)\n",
    "cf = count_izer.fit_transform(txts)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 173.940s.\n"
     ]
    }
   ],
   "source": [
    "# Now compute LDA\n",
    "lda = LatentDirichletAllocation(max_iter=5,\n",
    "                                learning_method='online', \n",
    "                                learning_offset=50.,\n",
    "                                random_state=0)\n",
    "t0 = time()\n",
    "lda.fit(cf)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topics in LDA model:\n",
      "Topic #0:\n",
      "image pp images based vol recognition using vol pp ieee face proposed used method fig detection processing features analysis different results\n",
      "Topic #1:\n",
      "matrix algorithm problem sparse rank convex optimization norm method linear function solution gradient methods matrices data algorithms using non vector\n",
      "Topic #2:\n",
      "learning network training neural layer networks model deep al input et et al word words using layers models neural networks language used\n",
      "Topic #3:\n",
      "data learning set classification training class feature kernel features classifier number using used based performance methods algorithm dataset test accuracy\n",
      "Topic #4:\n",
      "model data models distribution time number using variables parameters bayesian likelihood inference figure set al given information probability used et\n",
      "Topic #5:\n",
      "image images using figure time fig camera motion used reconstruction scale noise point filter error pixel data frame method space\n",
      "Topic #6:\n",
      "graph clustering points algorithm point cluster set data distance clusters edge figure number segmentation means method space edges based nodes\n",
      "Topic #7:\n",
      "let theorem proof log probability lemma set bound function algorithm xi case distribution following given random learning x1 sample definition\n",
      "Topic #8:\n",
      "image object images features model detection dataset recognition visual using based feature video al et al et vision objects results training\n",
      "Topic #9:\n",
      "learning algorithm xt time state policy regret function problem optimal action online bound value reward loss algorithms agent set case\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nTopics in LDA model:\")\n",
    "tf_feature_names = count_izer.get_feature_names()\n",
    "print_top_words(lda, tf_feature_names, n_top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
