{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load in libraries\n",
    "import cPickle as pickle\n",
    "import urllib2\n",
    "import shutil\n",
    "from time import time\n",
    "import os\n",
    "import random\n",
    "import io\n",
    "from __future__ import print_function\n",
    "\n",
    "# Data libraries\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "\n",
    "# ML libraries\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.decomposition import LatentDirichletAllocation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Data (raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# http://stackoverflow.com/questions/10058591/how-can-i-open-utf-16-files-on-python-2-x\n",
    "\n",
    "db = pickle.load(open('db.p', 'rb'))\n",
    "txts = []\n",
    "pids = []\n",
    "n=0\n",
    "for pid,j in db.iteritems():\n",
    "  n+=1\n",
    "  fname = os.path.join('txt', pid) + '.txt'\n",
    "  if os.path.isfile(fname):\n",
    "    try:\n",
    "        txt = io.open(fname, 'r', encoding = 'utf-16').read()\n",
    "        txts.append(txt)\n",
    "    except UnicodeError:\n",
    "        txt = open(fname, 'r').read()\n",
    "        txts.append(txt)\n",
    "    pids.append(pid)\n",
    "  #print 'reading %d/%d' % (n, len(db))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "txts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing txt.p\n"
     ]
    }
   ],
   "source": [
    "out = {}\n",
    "out['text'] = txts\n",
    "out['pids'] = pids\n",
    "\n",
    "print('writing txt.p')\n",
    "pickle.dump(out, open(\"txt.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Data (already processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "txt_db = pickle.load(open('txt.p', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "txts = txt_db['text']\n",
    "pids = txt_db['pids']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-733e19a20c93>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m                              \u001b[0mtoken_pattern\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mr'(?u)\\b[a-zA-Z_][a-zA-Z0-9_]+\\b'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m                              ngram_range=(1, 2), max_features = 5000)\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mcf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtxts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"done in %0.3fs.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mt0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/devnambi/anaconda/lib/python2.7/site-packages/sklearn/feature_extraction/text.pyc\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m    822\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    823\u001b[0m         vocabulary, X = self._count_vocab(raw_documents,\n\u001b[0;32m--> 824\u001b[0;31m                                           self.fixed_vocabulary_)\n\u001b[0m\u001b[1;32m    825\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    826\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/devnambi/anaconda/lib/python2.7/site-packages/sklearn/feature_extraction/text.pyc\u001b[0m in \u001b[0;36m_count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m    750\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mraw_documents\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    751\u001b[0m             \u001b[0mfeature_counter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 752\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[0;32min\u001b[0m \u001b[0manalyze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    753\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    754\u001b[0m                     \u001b[0mfeature_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/devnambi/anaconda/lib/python2.7/site-packages/sklearn/feature_extraction/text.pyc\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(doc)\u001b[0m\n\u001b[1;32m    239\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m             return lambda doc: self._word_ngrams(\n\u001b[0;32m--> 241\u001b[0;31m                 tokenize(preprocess(self.decode(doc))), stop_words)\n\u001b[0m\u001b[1;32m    242\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/devnambi/anaconda/lib/python2.7/site-packages/sklearn/feature_extraction/text.pyc\u001b[0m in \u001b[0;36m_word_ngrams\u001b[0;34m(self, tokens, stop_words)\u001b[0m\n\u001b[1;32m    139\u001b[0m                             min(max_n + 1, n_original_tokens + 1)):\n\u001b[1;32m    140\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mxrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_original_tokens\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m                     \u001b[0mtokens\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moriginal_tokens\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Use tf (raw term count) features for LDA.\n",
    "t0 = time()\n",
    "tf = CountVectorizer(input='content',\n",
    "                             encoding='utf-8', decode_error='replace', strip_accents='unicode', lowercase=True, \n",
    "                             analyzer='word', stop_words='english',\n",
    "                             token_pattern=r'(?u)\\b[a-zA-Z_][a-zA-Z0-9_]+\\b',\n",
    "                             ngram_range=(1, 2), max_features = 5000)\n",
    "cf = tf.fit_transform(txts)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "# completes in "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 703.415s.\n"
     ]
    }
   ],
   "source": [
    "# Now compute LDA\n",
    "lda = LatentDirichletAllocation(n_topics = 30,\n",
    "                                max_iter=10,\n",
    "                                learning_method='online', \n",
    "                                learning_offset=50.,\n",
    "                                random_state=0)\n",
    "t0 = time()\n",
    "lda.fit(tf)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "# done in 703.415s (12 minutes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topics in LDA model:\n",
      "Topic #0:\n",
      "pp vol vol pp signal ieee fig proposed performance based noise using algorithm processing ii information signals iii data trans ieee trans mean iv vector proc signal processing transactions channel rate sensing ieee transactions\n",
      "Topic #1:\n",
      "sparse norm matrix l1 sparsity rank problem low lasso recovery low rank l2 et dictionary al et al convex min solution robust regularization model group non subspace optimization minimization completion following zero\n",
      "Topic #2:\n",
      "word words language text model document al speech et sentence et al documents semantic models corpus using sentences sequence based used vector table use context training character set lstm vectors results\n",
      "Topic #3:\n",
      "xt online wt gradient stochastic algorithm yt ft learning zt update xt xt function gp algorithms batch using rate step descent optimization ht tt dt ut gt time sgd loss mt\n",
      "Topic #4:\n",
      "features feature data selection number set methods method test used using performance results al selected et al values analysis et accuracy based different figure table validation feature selection value high correlation information\n",
      "Topic #5:\n",
      "matrix rank matrices tensor vector columns vectors column algorithm decomposition data error entries spectral approximation covariance singular analysis linear row diagonal using svd random order low factorization method rows sampling\n",
      "Topic #6:\n",
      "points shape point figure edge segmentation set boundary space region curve fig given line surface regions local contour edges path shapes curves geometric defined section function order invariant distance graph\n",
      "Topic #7:\n",
      "bound log lemma theorem proof let algorithm probability learning bounds distribution sample random set following inequality case error complexity follows pr lower upper number result note given max function using\n",
      "Topic #8:\n",
      "user users data model items ranking based information item query number social set web acm quality pages figure results content dataset use different systems work performance rank ratings using queries\n",
      "Topic #9:\n",
      "regret algorithm time arm bound problem game bandit optimal player strategy reward expected set arms round best games loss case let et number proof probability learning theorem bounds al setting\n",
      "Topic #10:\n",
      "learning loss xi regression function training yi al et al svm et data linear problem vector machine margin prediction loss function optimization risk regularization using functions machine learning set logistic methods support error\n",
      "Topic #11:\n",
      "classification class classifier training classifiers set data learning error classes accuracy test decision ensemble cost algorithm performance label used number rate using positive examples svm boosting algorithms results dataset random\n",
      "Topic #12:\n",
      "learning data label supervised domain labels training multi target al et al et labeled task classification features dataset set feature different based class tasks source performance using information instance semi view\n",
      "Topic #13:\n",
      "x1 let theorem function proof xi xn x2 following functions lemma set x0 case defined definition follows proposition given section consider finite condition x1 x2 pr space define exists assume f1\n",
      "Topic #14:\n",
      "image images noise method using based results filter dictionary wavelet transform proposed processing pixels pixel reconstruction methods signal resolution algorithm denoising fig imaging ieee used original coefficients quality figure image processing\n",
      "Topic #15:\n",
      "model data variables distribution models structure estimation network bayesian likelihood probability estimate log set given variable conditional networks estimator parameters using learning parameter method figure estimates values maximum number sample\n",
      "Topic #16:\n",
      "distance data metric space dimensional points nearest similarity matching manifold set subspace pca dimension methods point search embedding neighbor method based local vectors dimensionality number vector using distances query euclidean\n",
      "Topic #17:\n",
      "kernel kernels space gaussian learning linear function data functions xi nonlinear al et al et machine based matrix hilbert rbf methods operator xj using vector positive dimensional machine learning laplacian feature product\n",
      "Topic #18:\n",
      "problem algorithm optimization convex solution function method xk convergence problems methods algorithms gradient iteration objective dual iterations optimal set step min linear cost constraints point non time constraint using case\n",
      "Topic #19:\n",
      "time data temporal series model time series event activity detection sequence events dynamic using prediction sequences models different change trajectories fig patterns activities based analysis figure length trajectory process used number\n",
      "Topic #20:\n",
      "model distribution models data log latent prior bayesian inference posterior parameters mixture likelihood gaussian sampling topic process variational using distributions mean probability al et al et number samples variables parameter given\n",
      "Topic #21:\n",
      "image object images features detection visual dataset cnn model objects using feature results scale cvpr performance level scene use segmentation vision training set ground based recognition different human pose method\n",
      "Topic #22:\n",
      "graph node nodes tree graphs edges edge algorithm et set trees inference xi energy random ij sum structure al variables submodular leaf function pairwise path propagation la et al vertices time\n",
      "Topic #23:\n",
      "network layer neural training networks deep learning input layers neural networks model al et al et hidden output trained convolutional neural network units using models architecture used figure weights arxiv data parameters error\n",
      "Topic #24:\n",
      "tracking camera motion frame image using figure depth object frames used time algorithm pixel point real video fig estimation pixels error scene light color vision background target results images optical\n",
      "Topic #25:\n",
      "state learning action policy agent value actions states function time reward model optimal control reinforcement reinforcement learning st agents space transition based algorithm decision problem environment markov policies dynamics task rl\n",
      "Topic #26:\n",
      "data algorithm used based using international new number systems rules learning table algorithms computer science machine set fuzzy journal research knowledge mining techniques level rule http applications time information figure\n",
      "Topic #27:\n",
      "information set case example probability theory given entropy possible problem function complexity general length number way section does definition measure new distribution finite defined order consider values model value simple\n",
      "Topic #28:\n",
      "clustering cluster data clusters algorithm means points number set ci algorithms partition distance method graph based data points spectral xi point results different similarity methods spectral clustering number clusters sets hierarchical dataset data set\n",
      "Topic #29:\n",
      "recognition image face images based features feature computer using ieee method detection pattern video used vision proposed segmentation computer vision pp fig analysis international different color database et al et al pattern recognition\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nTopics in LDA model:\")\n",
    "tf_feature_names = cv.get_feature_names()\n",
    "print_top_words(lda, tf_feature_names, n_top_words = 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Save the Topic Model\n",
    "\n",
    "* For each word, what is its score per topic\n",
    "* For each document, what is its score per topic\n",
    "* Which topic does each document belong to"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "model\n",
    "feature_names\n",
    "\n",
    "for topic_idx, topic in enumerate(model.components_):\n",
    "    print(\"Topic #%d:\" % topic_idx) #enumerate which topic number it is.\n",
    "    for i in topic.argsort()[:-30 - 1:-1]:\n",
    "        print feature_names[i]\n",
    "        \n",
    "\n",
    "    print(\" \".join([feature_names[i]\n",
    "                    for i in topic.argsort()[:-30:-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create a dataframe with topic weights for each word\n",
    "word_scores = pd.DataFrame(columns=['Word','Topic', 'Weight'])\n",
    "for topic_idx, topic in enumerate(lda.components_):\n",
    "    pd_to_add = pd.DataFrame( {'Word': tf_feature_names, \n",
    "                               'Topic': [str(topic_idx)] * len(tf_feature_names),\n",
    "                               'Weight': topic} )\n",
    "    word_scores = pd.concat([word_scores, pd_to_add])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a dataframe with docs to words. Use pids\n",
    "doc_to_word = pd.DataFrame(columns=['PID','Word', 'Count'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "doc_scratch = DataFrame(cf.A, columns=cv.get_feature_names())\n",
    "doc_scratch = doc_scratch.unstack()\n",
    "doc_scratch = doc_scratch.reset_index(name='value')\n",
    "doc_scratch.rename(columns={'level_0': 'Word', 'level_1': 'PidIndex', 'value': 'WordCount'}, inplace=True)\n",
    "doc_scratch = doc_scratch[doc_scratch['WordCount'] > 0]\n",
    "\n",
    "# doc_scratch.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>PidIndex</th>\n",
       "      <th>WordCount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>a0</td>\n",
       "      <td>19</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>a0</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>a0</td>\n",
       "      <td>74</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>a0</td>\n",
       "      <td>106</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>a0</td>\n",
       "      <td>126</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>a0</td>\n",
       "      <td>154</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>a0</td>\n",
       "      <td>168</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>a0</td>\n",
       "      <td>192</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204</th>\n",
       "      <td>a0</td>\n",
       "      <td>204</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205</th>\n",
       "      <td>a0</td>\n",
       "      <td>205</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219</th>\n",
       "      <td>a0</td>\n",
       "      <td>219</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245</th>\n",
       "      <td>a0</td>\n",
       "      <td>245</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>253</th>\n",
       "      <td>a0</td>\n",
       "      <td>253</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>267</th>\n",
       "      <td>a0</td>\n",
       "      <td>267</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>272</th>\n",
       "      <td>a0</td>\n",
       "      <td>272</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>273</th>\n",
       "      <td>a0</td>\n",
       "      <td>273</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>334</th>\n",
       "      <td>a0</td>\n",
       "      <td>334</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>339</th>\n",
       "      <td>a0</td>\n",
       "      <td>339</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>354</th>\n",
       "      <td>a0</td>\n",
       "      <td>354</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>380</th>\n",
       "      <td>a0</td>\n",
       "      <td>380</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>387</th>\n",
       "      <td>a0</td>\n",
       "      <td>387</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>398</th>\n",
       "      <td>a0</td>\n",
       "      <td>398</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>414</th>\n",
       "      <td>a0</td>\n",
       "      <td>414</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>454</th>\n",
       "      <td>a0</td>\n",
       "      <td>454</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>462</th>\n",
       "      <td>a0</td>\n",
       "      <td>462</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>482</th>\n",
       "      <td>a0</td>\n",
       "      <td>482</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>525</th>\n",
       "      <td>a0</td>\n",
       "      <td>525</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>614</th>\n",
       "      <td>a0</td>\n",
       "      <td>614</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>643</th>\n",
       "      <td>a0</td>\n",
       "      <td>643</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>668</th>\n",
       "      <td>a0</td>\n",
       "      <td>668</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63419635</th>\n",
       "      <td>zt</td>\n",
       "      <td>12319</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63419639</th>\n",
       "      <td>zt</td>\n",
       "      <td>12323</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63419650</th>\n",
       "      <td>zt</td>\n",
       "      <td>12334</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63419656</th>\n",
       "      <td>zt</td>\n",
       "      <td>12340</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63419662</th>\n",
       "      <td>zt</td>\n",
       "      <td>12346</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63419679</th>\n",
       "      <td>zt</td>\n",
       "      <td>12363</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63419706</th>\n",
       "      <td>zt</td>\n",
       "      <td>12390</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63419727</th>\n",
       "      <td>zt</td>\n",
       "      <td>12411</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63419739</th>\n",
       "      <td>zt</td>\n",
       "      <td>12423</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63419740</th>\n",
       "      <td>zt</td>\n",
       "      <td>12424</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63419747</th>\n",
       "      <td>zt</td>\n",
       "      <td>12431</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63419768</th>\n",
       "      <td>zt</td>\n",
       "      <td>12452</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63419797</th>\n",
       "      <td>zt</td>\n",
       "      <td>12481</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63419800</th>\n",
       "      <td>zt</td>\n",
       "      <td>12484</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63419832</th>\n",
       "      <td>zt</td>\n",
       "      <td>12516</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63419844</th>\n",
       "      <td>zt</td>\n",
       "      <td>12528</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63419849</th>\n",
       "      <td>zt</td>\n",
       "      <td>12533</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63419852</th>\n",
       "      <td>zt</td>\n",
       "      <td>12536</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63419868</th>\n",
       "      <td>zt</td>\n",
       "      <td>12552</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63419875</th>\n",
       "      <td>zt</td>\n",
       "      <td>12559</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63419893</th>\n",
       "      <td>zt</td>\n",
       "      <td>12577</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63419895</th>\n",
       "      <td>zt</td>\n",
       "      <td>12579</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63419903</th>\n",
       "      <td>zt</td>\n",
       "      <td>12587</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63419936</th>\n",
       "      <td>zt</td>\n",
       "      <td>12620</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63419941</th>\n",
       "      <td>zt</td>\n",
       "      <td>12625</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63419944</th>\n",
       "      <td>zt</td>\n",
       "      <td>12628</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63419956</th>\n",
       "      <td>zt</td>\n",
       "      <td>12640</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63419967</th>\n",
       "      <td>zt</td>\n",
       "      <td>12651</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63419972</th>\n",
       "      <td>zt</td>\n",
       "      <td>12656</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63419973</th>\n",
       "      <td>zt</td>\n",
       "      <td>12657</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10707054 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Word  PidIndex  WordCount\n",
       "19         a0        19          5\n",
       "70         a0        70          1\n",
       "74         a0        74          3\n",
       "106        a0       106          2\n",
       "126        a0       126          3\n",
       "154        a0       154          1\n",
       "168        a0       168          3\n",
       "192        a0       192          1\n",
       "204        a0       204          5\n",
       "205        a0       205          2\n",
       "219        a0       219          1\n",
       "245        a0       245          2\n",
       "253        a0       253          8\n",
       "267        a0       267          1\n",
       "272        a0       272          1\n",
       "273        a0       273          1\n",
       "334        a0       334          2\n",
       "339        a0       339          1\n",
       "354        a0       354          1\n",
       "380        a0       380          1\n",
       "387        a0       387          1\n",
       "398        a0       398          1\n",
       "414        a0       414          8\n",
       "454        a0       454          1\n",
       "462        a0       462          2\n",
       "482        a0       482          1\n",
       "525        a0       525          2\n",
       "614        a0       614          1\n",
       "643        a0       643          2\n",
       "668        a0       668          1\n",
       "...       ...       ...        ...\n",
       "63419635   zt     12319          6\n",
       "63419639   zt     12323          2\n",
       "63419650   zt     12334          2\n",
       "63419656   zt     12340          1\n",
       "63419662   zt     12346          9\n",
       "63419679   zt     12363         12\n",
       "63419706   zt     12390          3\n",
       "63419727   zt     12411          2\n",
       "63419739   zt     12423          6\n",
       "63419740   zt     12424         24\n",
       "63419747   zt     12431          1\n",
       "63419768   zt     12452          6\n",
       "63419797   zt     12481          1\n",
       "63419800   zt     12484          7\n",
       "63419832   zt     12516          4\n",
       "63419844   zt     12528          4\n",
       "63419849   zt     12533          6\n",
       "63419852   zt     12536          1\n",
       "63419868   zt     12552          6\n",
       "63419875   zt     12559          1\n",
       "63419893   zt     12577          2\n",
       "63419895   zt     12579          4\n",
       "63419903   zt     12587         18\n",
       "63419936   zt     12620          1\n",
       "63419941   zt     12625          1\n",
       "63419944   zt     12628          2\n",
       "63419956   zt     12640          1\n",
       "63419967   zt     12651          1\n",
       "63419972   zt     12656          6\n",
       "63419973   zt     12657          2\n",
       "\n",
       "[10707054 rows x 3 columns]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-80-87d1e74147fa>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdoc_weights\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc_scratch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword_scores\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhow\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'inner'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mon\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'Word'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Users\\Dev Nambi\\Anaconda2\\lib\\site-packages\\pandas\\tools\\merge.py\u001b[0m in \u001b[0;36mmerge\u001b[1;34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator)\u001b[0m\n\u001b[0;32m     33\u001b[0m                          \u001b[0mright_index\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mright_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msort\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msort\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msuffixes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msuffixes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m                          copy=copy, indicator=indicator)\n\u001b[1;32m---> 35\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     36\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__debug__\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m     \u001b[0mmerge\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__doc__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_merge_doc\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;34m'\\nleft : DataFrame'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Dev Nambi\\Anaconda2\\lib\\site-packages\\pandas\\tools\\merge.py\u001b[0m in \u001b[0;36mget_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    208\u001b[0m             \u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mldata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlindexers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mrdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrindexers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    209\u001b[0m             \u001b[0maxes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mllabels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mjoin_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 210\u001b[1;33m             concat_axis=0, copy=self.copy)\n\u001b[0m\u001b[0;32m    211\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    212\u001b[0m         \u001b[0mtyp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mleft\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_constructor\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Dev Nambi\\Anaconda2\\lib\\site-packages\\pandas\\core\\internals.py\u001b[0m in \u001b[0;36mconcatenate_block_managers\u001b[1;34m(mgrs_indexers, axes, concat_axis, copy)\u001b[0m\n\u001b[0;32m   4454\u001b[0m                                                 copy=copy),\n\u001b[0;32m   4455\u001b[0m                          placement=placement)\n\u001b[1;32m-> 4456\u001b[1;33m               for placement, join_units in concat_plan]\n\u001b[0m\u001b[0;32m   4457\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4458\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mBlockManager\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mblocks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Dev Nambi\\Anaconda2\\lib\\site-packages\\pandas\\core\\internals.py\u001b[0m in \u001b[0;36mconcatenate_join_units\u001b[1;34m(join_units, concat_axis, copy)\u001b[0m\n\u001b[0;32m   4557\u001b[0m         \u001b[0mconcat_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_concat\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4558\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcopy\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mconcat_values\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4559\u001b[1;33m             \u001b[0mconcat_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconcat_values\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4560\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4561\u001b[0m         \u001b[0mconcat_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_concat_compat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mto_concat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconcat_axis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "doc_weights = pd.merge(doc_scratch, word_scores, how='inner', on='Word')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "doc_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic</th>\n",
       "      <th>Weight</th>\n",
       "      <th>Word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>14.357561</td>\n",
       "      <td>a0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>118.602701</td>\n",
       "      <td>a1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>83.979146</td>\n",
       "      <td>a1 a2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>188.381372</td>\n",
       "      <td>a2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>36.414756</td>\n",
       "      <td>a3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>12.817502</td>\n",
       "      <td>aa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0.033335</td>\n",
       "      <td>aaai</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>168.789133</td>\n",
       "      <td>ab</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>0.033334</td>\n",
       "      <td>abc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>139.746212</td>\n",
       "      <td>ability</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "      <td>196.899345</td>\n",
       "      <td>able</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0</td>\n",
       "      <td>259.650396</td>\n",
       "      <td>abs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0</td>\n",
       "      <td>135.868500</td>\n",
       "      <td>absence</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0</td>\n",
       "      <td>187.111920</td>\n",
       "      <td>absolute</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0</td>\n",
       "      <td>277.853295</td>\n",
       "      <td>abstract</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0</td>\n",
       "      <td>209.269978</td>\n",
       "      <td>abstract paper</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0</td>\n",
       "      <td>0.033334</td>\n",
       "      <td>abstraction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0</td>\n",
       "      <td>49.895217</td>\n",
       "      <td>ac</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0</td>\n",
       "      <td>26.751219</td>\n",
       "      <td>ac uk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0</td>\n",
       "      <td>196.480958</td>\n",
       "      <td>academic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0</td>\n",
       "      <td>61.191075</td>\n",
       "      <td>academy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0</td>\n",
       "      <td>6.800344</td>\n",
       "      <td>acc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0</td>\n",
       "      <td>0.033469</td>\n",
       "      <td>accelerated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0</td>\n",
       "      <td>41.864808</td>\n",
       "      <td>acceleration</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0</td>\n",
       "      <td>56.963325</td>\n",
       "      <td>acceptable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0</td>\n",
       "      <td>86.963222</td>\n",
       "      <td>accepted</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0</td>\n",
       "      <td>757.325114</td>\n",
       "      <td>access</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0</td>\n",
       "      <td>1700.781336</td>\n",
       "      <td>according</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0</td>\n",
       "      <td>328.389993</td>\n",
       "      <td>accordingly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0</td>\n",
       "      <td>373.692483</td>\n",
       "      <td>account</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4970</th>\n",
       "      <td>29</td>\n",
       "      <td>58.221796</td>\n",
       "      <td>york</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4971</th>\n",
       "      <td>29</td>\n",
       "      <td>0.488636</td>\n",
       "      <td>york ny</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4972</th>\n",
       "      <td>29</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>yoshua</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4973</th>\n",
       "      <td>29</td>\n",
       "      <td>99.459991</td>\n",
       "      <td>young</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4974</th>\n",
       "      <td>29</td>\n",
       "      <td>0.033371</td>\n",
       "      <td>yt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4975</th>\n",
       "      <td>29</td>\n",
       "      <td>0.033334</td>\n",
       "      <td>yt xt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4976</th>\n",
       "      <td>29</td>\n",
       "      <td>0.033334</td>\n",
       "      <td>yt yt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4977</th>\n",
       "      <td>29</td>\n",
       "      <td>295.040693</td>\n",
       "      <td>yu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4978</th>\n",
       "      <td>29</td>\n",
       "      <td>221.804275</td>\n",
       "      <td>yuan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4979</th>\n",
       "      <td>29</td>\n",
       "      <td>0.033417</td>\n",
       "      <td>yx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4980</th>\n",
       "      <td>29</td>\n",
       "      <td>0.033397</td>\n",
       "      <td>z0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4981</th>\n",
       "      <td>29</td>\n",
       "      <td>0.033484</td>\n",
       "      <td>z1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4982</th>\n",
       "      <td>29</td>\n",
       "      <td>0.033340</td>\n",
       "      <td>z1 z2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4983</th>\n",
       "      <td>29</td>\n",
       "      <td>0.033342</td>\n",
       "      <td>z2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4984</th>\n",
       "      <td>29</td>\n",
       "      <td>36.521563</td>\n",
       "      <td>zero</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4985</th>\n",
       "      <td>29</td>\n",
       "      <td>0.051954</td>\n",
       "      <td>zero mean</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4986</th>\n",
       "      <td>29</td>\n",
       "      <td>0.386362</td>\n",
       "      <td>zeros</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4987</th>\n",
       "      <td>29</td>\n",
       "      <td>2140.911026</td>\n",
       "      <td>zhang</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4988</th>\n",
       "      <td>29</td>\n",
       "      <td>569.409982</td>\n",
       "      <td>zhao</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4989</th>\n",
       "      <td>29</td>\n",
       "      <td>522.278339</td>\n",
       "      <td>zhou</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4990</th>\n",
       "      <td>29</td>\n",
       "      <td>417.445182</td>\n",
       "      <td>zhu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4991</th>\n",
       "      <td>29</td>\n",
       "      <td>1.226484</td>\n",
       "      <td>zi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4992</th>\n",
       "      <td>29</td>\n",
       "      <td>0.033598</td>\n",
       "      <td>zi zi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4993</th>\n",
       "      <td>29</td>\n",
       "      <td>65.929796</td>\n",
       "      <td>zisserman</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4994</th>\n",
       "      <td>29</td>\n",
       "      <td>0.036936</td>\n",
       "      <td>zj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4995</th>\n",
       "      <td>29</td>\n",
       "      <td>0.053065</td>\n",
       "      <td>zk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4996</th>\n",
       "      <td>29</td>\n",
       "      <td>0.033990</td>\n",
       "      <td>zl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4997</th>\n",
       "      <td>29</td>\n",
       "      <td>20.632443</td>\n",
       "      <td>zm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4998</th>\n",
       "      <td>29</td>\n",
       "      <td>0.033385</td>\n",
       "      <td>zn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4999</th>\n",
       "      <td>29</td>\n",
       "      <td>0.035693</td>\n",
       "      <td>zt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150000 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Topic       Weight            Word\n",
       "0        0    14.357561              a0\n",
       "1        0   118.602701              a1\n",
       "2        0    83.979146           a1 a2\n",
       "3        0   188.381372              a2\n",
       "4        0    36.414756              a3\n",
       "5        0    12.817502              aa\n",
       "6        0     0.033335            aaai\n",
       "7        0   168.789133              ab\n",
       "8        0     0.033334             abc\n",
       "9        0   139.746212         ability\n",
       "10       0   196.899345            able\n",
       "11       0   259.650396             abs\n",
       "12       0   135.868500         absence\n",
       "13       0   187.111920        absolute\n",
       "14       0   277.853295        abstract\n",
       "15       0   209.269978  abstract paper\n",
       "16       0     0.033334     abstraction\n",
       "17       0    49.895217              ac\n",
       "18       0    26.751219           ac uk\n",
       "19       0   196.480958        academic\n",
       "20       0    61.191075         academy\n",
       "21       0     6.800344             acc\n",
       "22       0     0.033469     accelerated\n",
       "23       0    41.864808    acceleration\n",
       "24       0    56.963325      acceptable\n",
       "25       0    86.963222        accepted\n",
       "26       0   757.325114          access\n",
       "27       0  1700.781336       according\n",
       "28       0   328.389993     accordingly\n",
       "29       0   373.692483         account\n",
       "...    ...          ...             ...\n",
       "4970    29    58.221796            york\n",
       "4971    29     0.488636         york ny\n",
       "4972    29     0.033333          yoshua\n",
       "4973    29    99.459991           young\n",
       "4974    29     0.033371              yt\n",
       "4975    29     0.033334           yt xt\n",
       "4976    29     0.033334           yt yt\n",
       "4977    29   295.040693              yu\n",
       "4978    29   221.804275            yuan\n",
       "4979    29     0.033417              yx\n",
       "4980    29     0.033397              z0\n",
       "4981    29     0.033484              z1\n",
       "4982    29     0.033340           z1 z2\n",
       "4983    29     0.033342              z2\n",
       "4984    29    36.521563            zero\n",
       "4985    29     0.051954       zero mean\n",
       "4986    29     0.386362           zeros\n",
       "4987    29  2140.911026           zhang\n",
       "4988    29   569.409982            zhao\n",
       "4989    29   522.278339            zhou\n",
       "4990    29   417.445182             zhu\n",
       "4991    29     1.226484              zi\n",
       "4992    29     0.033598           zi zi\n",
       "4993    29    65.929796       zisserman\n",
       "4994    29     0.036936              zj\n",
       "4995    29     0.053065              zk\n",
       "4996    29     0.033990              zl\n",
       "4997    29    20.632443              zm\n",
       "4998    29     0.033385              zn\n",
       "4999    29     0.035693              zt\n",
       "\n",
       "[150000 rows x 3 columns]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pids = txt_db['pids']\n",
    "pd.merge(word_scores, )\n",
    "[feature_names[i]\n",
    " cf = tf.fit_transform(txts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'accuracies'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.get_feature_names()[30]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Word       accuracies\n",
       "Topic0        3.25047\n",
       "Topic1       0.075337\n",
       "Topic2        107.479\n",
       "Topic3      0.0383341\n",
       "Topic4        617.372\n",
       "Topic5      0.0478734\n",
       "Topic6      0.0333353\n",
       "Topic7      0.0333339\n",
       "Topic8        5.75345\n",
       "Topic9      0.0333448\n",
       "Topic10        283.35\n",
       "Topic11       814.822\n",
       "Topic12       535.882\n",
       "Topic13     0.0333589\n",
       "Topic14      0.211527\n",
       "Topic15     0.0337312\n",
       "Topic16       400.844\n",
       "Topic17       81.9922\n",
       "Topic18     0.0341962\n",
       "Topic19      0.158811\n",
       "Topic20     0.0339305\n",
       "Topic21       292.875\n",
       "Topic22     0.0334796\n",
       "Topic23       198.575\n",
       "Topic24     0.0333528\n",
       "Topic25     0.0333464\n",
       "Topic26     0.0342419\n",
       "Topic27     0.0333338\n",
       "Topic28       8.76951\n",
       "Topic29       140.629\n",
       "Name: 30, dtype: object"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_scores.ix[30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save tf\n",
    "out = {}\n",
    "out['word'] = txts\n",
    "out['counts'] = pids\n",
    "\n",
    "print('writing count_vectors.p')\n",
    "pickle.dump(out, open(\"count_vectors.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save LDA\n",
    "lda\n",
    "\n",
    "out = {}\n",
    "out['word'] = txts\n",
    "out['counts'] = pids\n",
    "\n",
    "print('writing count_vectors.p')\n",
    "pickle.dump(out, open(\"count_vectors.p\", \"wb\"))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
